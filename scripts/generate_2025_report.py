import pandas as pd
import json
import os
import datetime

# è·¯å¾„é…ç½®
JSON_PATH = "chat_data.json"
MEDIA_DIR = r"Doppelganger/dxağŸ¥°_files"
OUTPUT_REPORT = "2025å¹´åº¦èŠå¤©æŠ¥å‘Š.md"

def get_file_stats(directory):
    total_size = 0
    file_counts = {'å›¾ç‰‡': 0, 'è§†é¢‘': 0, 'è¯­éŸ³': 0, 'å…¶ä»–': 0}
    if not os.path.exists(directory):
        return 0, file_counts
    for root, dirs, files in os.walk(directory):
        for file in files:
            fp = os.path.join(root, file)
            try:
                size = os.path.getsize(fp)
                total_size += size
                ext = file.lower().split('.')[-1]
                if ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']:
                    file_counts['å›¾ç‰‡'] += 1
                elif ext in ['mp4', 'mov', 'avi', 'mkv']:
                    file_counts['è§†é¢‘'] += 1
                elif ext in ['mp3', 'wav', 'aac', 'amr', 'silk']:
                    file_counts['è¯­éŸ³'] += 1
                else:
                    file_counts['å…¶ä»–'] += 1
            except:
                pass
    return total_size, file_counts

def generate_report():
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    try:
        with open(JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        df = pd.DataFrame(data)
    except Exception as e:
        print(f"åŠ è½½ JSON å¤±è´¥: {e}")
        return

    # --- 1. æ•°æ®æ¸…æ´—ä¸æ˜ å°„ ---
    # å¼ºåˆ¶æ˜ å°„å‘é€è€…
    df['sender'] = df['alignment'].map({'left': 'dxa', 'right': 'lxg'}).fillna('ç³»ç»Ÿæ¶ˆæ¯')
    
    # æ—¶é—´è½¬æ¢
    df['dt'] = pd.to_datetime(df['timestamp_raw'], format='%Y-%m-%d %I:%M:%S %p', errors='coerce')
    
    # æ’åº
    df['msg_id'] = pd.to_numeric(df['msg_id'], errors='coerce')
    df = df.sort_values('msg_id')

    # é”å®šå¹´åº¦
    target_year = 2025
    year_df = df[df['dt'].dt.year == target_year].copy()

    # --- æ¨¡å— A: æ—¶å…‰å›æº¯ (å…¨é‡) ---
    first_msg = df.iloc[0]
    last_msg = df.iloc[-1]
    total_days = (last_msg['dt'] - first_msg['dt']).days if pd.notnull(first_msg['dt']) and pd.notnull(last_msg['dt']) else "æœªçŸ¥"
    
    # --- æ¨¡å— B: æ•°å­—åŒ–é‡é‡ (æ–‡ä»¶ç³»ç»Ÿ) ---
    total_bytes, file_counts = get_file_stats(MEDIA_DIR)
    total_mb = total_bytes / (1024 * 1024)

    # --- æ¨¡å— C: 2025å¹´åº¦æ´»è·ƒåº¦ ---
    if not year_df.empty:
        daily_counts = year_df.groupby(year_df['dt'].dt.date).size()
        peak_day = daily_counts.idxmax()
        peak_count = daily_counts.max()
        active_days = len(daily_counts)
        daily_avg = int(daily_counts.mean())
    else:
        peak_day, peak_count, active_days, daily_avg = "æ— æ•°æ®", 0, 0, 0

    # --- æ¨¡å— D: ä¹ æƒ¯é›·è¾¾ (æ¶ˆæ¯ç±»å‹åˆ†å¸ƒ) ---
    def map_type_cn(t):
        t = str(t)
        if t == '1': return 'æ–‡å­—'
        if t == 'image': return 'å›¾ç‰‡'
        if t == 'video': return 'è§†é¢‘'
        if t == '34': return 'è¯­éŸ³'
        if t == '47': return 'è¡¨æƒ…åŒ…'
        if t == '49': return 'é“¾æ¥/åº”ç”¨'
        if t == '43': return 'è§†é¢‘é€šè¯'
        return 'å…¶ä»–'
    
    df['ç±»å‹'] = df['msg_type'].apply(map_type_cn)
    radar = df[df['sender'].isin(['dxa', 'lxg'])].groupby(['sender', 'ç±»å‹']).size().unstack(fill_value=0)

    # --- æ¨¡å— E: å¹´åº¦æ€»ç»“ ---
    total_msgs_year = len(year_df)
    year_text_df = year_df[year_df['msg_type'] == '1']
    total_chars_year = year_text_df['content'].fillna("").apply(len).sum()

    # --- å†™å…¥æŠ¥å‘Š ---
    report_content = f"""# ğŸ® 2025å¹´åº¦èŠå¤©æŠ¥å‘Šï¼šMemoryLane

## ğŸ•°ï¸ æ¨¡å—ä¸€ï¼šæ—¶å…‰å›æº¯ (The Origin Story)
*   **ç¬¬ä¸€æ¡æ¶ˆæ¯ ID**: `{first_msg['msg_id']}`
*   **æ—¶é—´**: {first_msg['timestamp_raw']}
*   **å‘é€è€…**: **{first_msg['sender']}**
*   **å†…å®¹**: `{first_msg['content']}`
*   **ç¾ç»Šå¤©æ•°**: ä½ ä»¬å·²ç»å…±åŒèµ°è¿‡äº† **{total_days}** å¤©ã€‚

---

## ğŸ’¾ æ¨¡å—äºŒï¼šæ•°å­—åŒ–é‡é‡ (Digital Weight)
*   **æ€»å­˜å‚¨å ç”¨**: **{total_mb:.2f} MB**
*   **æ–‡ä»¶ç»Ÿè®¡**:
    *   ğŸ“¸ å›¾ç‰‡: {file_counts['å›¾ç‰‡']} å¼ 
    *   ğŸ¥ è§†é¢‘: {file_counts['è§†é¢‘']} ä¸ª
    *   ğŸ¤ è¯­éŸ³: {file_counts['è¯­éŸ³']} æ¡
*   **å…·è±¡åŒ–**: ä½ ä»¬çš„å›å¿†å¤§çº¦ç›¸å½“äº **{int(total_mb / 2)}** å¼ é«˜æ¸…ç…§ç‰‡ï¼Œæˆ– **{total_mb / 2500:.2f}** éƒ¨è¶…æ¸…ç”µå½±ã€‚

---

## ğŸ“… æ¨¡å—ä¸‰ï¼š2025å¹´åº¦æ—¥å†çƒ­åŠ›å›¾
*   **æ´»è·ƒå¤©æ•°**: 2025å¹´å…±æœ‰ **{active_days}** å¤©åœ¨èŠå¤© (å…¨å¹´å æ¯” {active_days/365:.1%})
*   **æ—¥å‡é¢‘ç‡**: å¹³å‡æ¯å¤©äº’å‘ **{daily_avg}** æ¡æ¶ˆæ¯
*   **å¹´åº¦æœ€çƒ­ä¸€å¤©**: **{peak_day}**
    *   é‚£ä¸€å¤©ï¼Œä½ ä»¬ç–¯ç‹‚èŠäº† **{peak_count}** æ¡æ¶ˆæ¯ã€‚

---

## ğŸ“¡ æ¨¡å—å››ï¼šä¹ æƒ¯é›·è¾¾ (æ²Ÿé€šé£æ ¼)
**ä¸¤ä½æˆå‘˜çš„æ¶ˆæ¯åå¥½åˆ†å¸ƒï¼š**

{radar.to_markdown()}

---

## ğŸ“ˆ æ¨¡å—äº”ï¼š2025å¹´åº¦æ€»ç»“æˆåˆ†è¡¨
*   **å¹´åº¦æ€»æ¶ˆæ¯æ•°**: {total_msgs_year} æ¡
*   **å¹´åº¦æ€»å­—æ•°**: {total_chars_year} å­—
*   **å†å²æ€»æ¶ˆæ¯æ•°**: {len(df)} æ¡
*   **æœ€çˆ±ç”¨çš„è¡¨è¾¾**: (ç­‰å¾…è¯äº‘åˆ†æ...)

---

**æŠ¥å‘Šè¯´æ˜**: æœ¬æŠ¥å‘Šå®Œå…¨åœ¨æœ¬åœ°ç”Ÿæˆï¼Œç¡®ä¿éšç§å®‰å…¨ã€‚
"""
    print("æŠ¥å‘Šç”Ÿæˆä¸­...")
    with open(OUTPUT_REPORT, "w", encoding="utf-8-sig") as f:
        f.write(report_content)
    print(f"å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {OUTPUT_REPORT}")

if __name__ == "__main__":
    generate_report()
